{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark/spark-2.2.0-bin-hadoop2.7/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((1, 4), (6, 2)), (9, 10))\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local[3]', 'three tasks') \n",
    "\n",
    "# we define a list of integers\n",
    "numbers = [1, 4, 6, 2, 9, 10]\n",
    "\n",
    "rdd_numbers=sc.parallelize(numbers)\n",
    "rdd_reduce = rdd_numbers.reduce(lambda x,y : \"(\" + str(x) +\", \" + str(y) +\")\")\n",
    "print(rdd_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL with JSON input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create Spark Data Frame\n",
    "players = sqlc.read.json(\"temp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----------+-----------+-----------------+---------+------+--------+---------+----+\n",
      "|                Club|ClubCountry|Competition|DateOfBirth|         FullName|IsCaptain|Number|Position|     Team|Year|\n",
      "+--------------------+-----------+-----------+-----------+-----------------+---------+------+--------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|  Argentina|  World Cup|   1905-5-5|     Ãngel Bossio|    false|      |      GK|Argentina|1930|\n",
      "|Quilmes AtlÃ©tico...|  Argentina|  World Cup| 1908-10-23|     Juan Botasso|    false|      |      GK|Argentina|1930|\n",
      "|          Boca Junio|  Argentina|  World Cup|  1907-2-23|   Roberto Cherro|    false|      |      FW|Argentina|1930|\n",
      "|Central Norte TucumÃ|  Argentina|  World Cup|  1907-2-23|Alberto Chividini|    false|      |      DF|Argentina|1930|\n",
      "|Club Atletico Est...|  Argentina|  World Cup|  1909-3-19|                 |    false|    10|      FW|Argentina|1930|\n",
      "+--------------------+-----------+-----------+-----------+-----------------+---------+------+--------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players.registerTempTable(\"T_people\")\n",
    "sqlc.sql(\"select * from T_people\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----------+-----------+-----------------+---------+------+--------+---------+----+\n",
      "|                Club|ClubCountry|Competition|DateOfBirth|         FullName|IsCaptain|Number|Position|     Team|Year|\n",
      "+--------------------+-----------+-----------+-----------+-----------------+---------+------+--------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|  Argentina|  World Cup|   1905-5-5|     Ãngel Bossio|    false|      |      GK|Argentina|1930|\n",
      "|Quilmes AtlÃ©tico...|  Argentina|  World Cup| 1908-10-23|     Juan Botasso|    false|      |      GK|Argentina|1930|\n",
      "|          Boca Junio|  Argentina|  World Cup|  1907-2-23|   Roberto Cherro|    false|      |      FW|Argentina|1930|\n",
      "|Central Norte TucumÃ|  Argentina|  World Cup|  1907-2-23|Alberto Chividini|    false|      |      DF|Argentina|1930|\n",
      "|Club Atletico Est...|  Argentina|  World Cup|  1909-3-19|                 |    false|    10|      FW|Argentina|1930|\n",
      "+--------------------+-----------+-----------+-----------+-----------------+---------+------+--------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Ãngel Bossio\n",
      "Name: Roberto Cherro\n",
      "Name: Juan Botasso\n",
      "Name: \n",
      "Name: Alberto Chividini\n",
      "Name: Juan Evaristo\n"
     ]
    }
   ],
   "source": [
    "# Select the player names from 1930 only \n",
    "team1930 = sqlc.sql(\"select distinct FullName from T_people where Year == 1930\")\n",
    "#\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd method returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "playerNames = team1930.rdd.map(lambda p: \"Name: \" + p.FullName).collect()\n",
    "for name in playerNames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Club</th>\n",
       "      <th>ClubCountry</th>\n",
       "      <th>Competition</th>\n",
       "      <th>DateOfBirth</th>\n",
       "      <th>FullName</th>\n",
       "      <th>IsCaptain</th>\n",
       "      <th>Number</th>\n",
       "      <th>Position</th>\n",
       "      <th>Team</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Club AtlÃ©tico Talleres de Remedios de Escalada</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>World Cup</td>\n",
       "      <td>1905-5-5</td>\n",
       "      <td>Ãngel Bossio</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>GK</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quilmes AtlÃ©tico Club</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>World Cup</td>\n",
       "      <td>1908-10-23</td>\n",
       "      <td>Juan Botasso</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>GK</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boca Junio</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>World Cup</td>\n",
       "      <td>1907-2-23</td>\n",
       "      <td>Roberto Cherro</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>FW</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Central Norte TucumÃ</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>World Cup</td>\n",
       "      <td>1907-2-23</td>\n",
       "      <td>Alberto Chividini</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>DF</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Club Atletico Estudiantil PorteÃ±o</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>World Cup</td>\n",
       "      <td>1909-3-19</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>FW</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Club ClubCountry Competition  \\\n",
       "0  Club AtlÃ©tico Talleres de Remedios de Escalada   Argentina   World Cup   \n",
       "1                           Quilmes AtlÃ©tico Club   Argentina   World Cup   \n",
       "2                                       Boca Junio   Argentina   World Cup   \n",
       "3                             Central Norte TucumÃ   Argentina   World Cup   \n",
       "4               Club Atletico Estudiantil PorteÃ±o   Argentina   World Cup   \n",
       "\n",
       "  DateOfBirth           FullName  IsCaptain Number Position       Team  Year  \n",
       "0    1905-5-5       Ãngel Bossio      False              GK  Argentina  1930  \n",
       "1  1908-10-23       Juan Botasso      False              GK  Argentina  1930  \n",
       "2   1907-2-23     Roberto Cherro      False              FW  Argentina  1930  \n",
       "3   1907-2-23  Alberto Chividini      False              DF  Argentina  1930  \n",
       "4   1909-3-19                         False     10       FW  Argentina  1930  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Data Frame is converted to Pandas Data Frame\n",
    "players.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(FullName='Ãngel Bossio')\n",
      "Row(FullName='Roberto Cherro')\n",
      "Row(FullName='Juan Botasso')\n",
      "Row(FullName='')\n",
      "Row(FullName='Alberto Chividini')\n",
      "Row(FullName='Juan Evaristo')\n"
     ]
    }
   ],
   "source": [
    "# do step 23 in an alternative way\n",
    "for b in team1930.collect(): print (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, read from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.supergloo.com/fieldnotes/spark-sql-csv-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlc.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('Uber-Jan-Feb-FOIL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"uber\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------+---------------+-----+----+\n",
      "|dispatching_base_number|    date|active_vehicles|trips| _c4|\n",
      "+-----------------------+--------+---------------+-----+----+\n",
      "|                 B02512|1/1/2015|            190| 1132|null|\n",
      "|                 B02765|1/1/2015|            225| 1765|null|\n",
      "|                 B02764|1/1/2015|           3427|29421|null|\n",
      "|                 B02682|1/1/2015|            945| 7679|null|\n",
      "|                 B02617|1/1/2015|           1228| 9537|null|\n",
      "+-----------------------+--------+---------------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_number: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- active_vehicles: integer (nullable = true)\n",
      " |-- trips: integer (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# it might be handy to know the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------+\n",
      "|dispatching_base_number|    cnt|\n",
      "+-----------------------+-------+\n",
      "|                 B02764|1914449|\n",
      "|                 B02617| 725025|\n",
      "|                 B02682| 662509|\n",
      "|                 B02598| 540791|\n",
      "|                 B02765| 193670|\n",
      "|                 B02512|  93786|\n",
      "+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# more advanced SQL, such as determining which Uber bases is the busiest based on number of trips\n",
    "busiestUber = sqlc.sql(\"\"\"select distinct(`dispatching_base_number`), \n",
    "                                sum(`trips`) as cnt from uber group by `dispatching_base_number` \n",
    "                                order by cnt desc\"\"\")\n",
    "busiestUber.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     date|   cnt|\n",
      "+---------+------+\n",
      "|2/20/2015|100915|\n",
      "|2/14/2015|100345|\n",
      "|2/21/2015| 98380|\n",
      "|2/13/2015| 98024|\n",
      "|1/31/2015| 92257|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# busiest date\n",
    "busiestDate = sqlc.sql(\"\"\"select distinct(`date`), \n",
    "                                sum(`trips`) as cnt from uber group by `date` \n",
    "                                order by cnt desc limit 5\"\"\")\n",
    "busiestDate.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connecting SQL Server\n",
    "https://stackoverflow.com/questions/43946157/pyspark-to-read-data-from-sql-server\n",
    "\n",
    "   query = \"(SELECT top 10 * from users) as users\"\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "df = sqlc.read.format(\"jdbc\").options(url=\"jdbc:sqlserver://mssqlserver:1433;database=user_management;user=pyspark;password=pyspark\", dbtable=query).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### connecting MYSQL\n",
    " \n",
    "    dataframe_mysql = sqlc.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost/uber\").option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"trips\").option(\"user\", \"root\").option(\"password\", \"root\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Apache Spark to connect to SQL Server, extract a table from SQL Server, and load the extracted rows into a Hive table:\n",
    "https://community.hortonworks.com/articles/59205/spark-pyspark-to-extract-from-sql-server.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    " \n",
    "conf = (SparkConf()\n",
    "  .setAppName(\"data_import\")\n",
    "  .set(\"spark.dynamicAllocation.enabled\",\"true\")\n",
    "  .set(\"spark.shuffle.service.enabled\",\"true\"))\n",
    "sc = SparkContext(conf = conf)\n",
    " \n",
    "sqlctx = HiveContext(sc)\n",
    " \n",
    "df = sqlctx.load(\n",
    "  source=\"jdbc\", \n",
    "  url=\"jdbc:sqlserver://ec2-54-244-44-6.us-west-2.compute.amazonaws.com:1433;database=sales;user=my_username;password=my_password\",\n",
    "  dbtable=\"orders\")\n",
    " \n",
    " \n",
    "## this is how to write to an ORC file\n",
    "# ORC format: An Intelligent Big Data file format for Hadoop and Hive\n",
    "\n",
    "df.write.format(\"orc\").save(\"/tmp/orc_query_output\")\n",
    " \n",
    "## this is how to write to a hive table\n",
    "df.write.mode('overwrite').format('orc').saveAsTable(\"test_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from Hive and write toJDBC Data Source (PostgreSQL table)\n",
    "http://qiita.com/jlyoung/items/1103ae5f4ca7e05c8e2e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"pySpark Hive JDBC Demo App\")\n",
    "# Create a Hive Context\n",
    "hive_context = HiveContext(sc)\n",
    "\n",
    "# Read from the Hive table \"crime\" on the \"default\" Hive database. The result is a DataFrame.\n",
    "print \"Reading Hive table...\"\n",
    "crime = hive_context.table(\"default.crime\")\n",
    "\n",
    "# Register the DataFrame crime as a temporary table crime_temp\n",
    "print \"Registering DataFrame as a table...\"\n",
    "crime.registerTempTable(\"crime_temp\")\n",
    "\n",
    "# Executing an SQL query over this temporary table to get a list of thefts of property worth less than $500USD.\n",
    "# The results will be another DataFrame.\n",
    "print \"Executing SQL SELECT query on DataFrame registered as a temporary table...\"\n",
    "pettythefts = hive_context.sql('SELECT * FROM crime_temp WHERE Primary_Type = \"THEFT\" AND Description = \"$500 AND UNDER\"')\n",
    "\n",
    "# Create a new DataFrame containing only the columns we wish to write to the JDBC connected datasource using \n",
    "print \"Creating a DataFrame of only the columns of our resultset to be persisted to JDBC DataSource...\"\n",
    "pettythefts_table_df = pettythefts.select(\"id\", \"case_number\", \"primary_type\", \"description\", \"location_description\", \"beat\", \"district\", \"ward\", \"community_area\")\n",
    "\n",
    "# Prepare the connection properties for the JDBC datasource.\n",
    "# table name is public.prettytheft\n",
    "mode = 'overwrite'\n",
    "url = 'jdbc:postgresql://<database server IP address>:5432/postgres?searchpath=public'\n",
    "properties = {\"user\": \"<username>\", \"password\": \"<password>\", \"driver\": \"org.postgresql.Driver\"}\n",
    "table = 'public.pettytheft'\n",
    "\n",
    "# Write the contents of the DataFrame to the JDBC datasource (Postgres) using the connection URL defined above.\n",
    "print \"Writing DataFrame to JDBC Datasource...\"\n",
    "pettythefts_table_df.write.jdbc(url=url, table=table, mode=mode, properties=properties)\n",
    "\n",
    "print \"Exiting...\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
